{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitaanala/Resume-parser/blob/main/Resume_parser_with_Regex_nlp_mini_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z26jzcLniKGo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq7YBV7Rn25L",
        "outputId": "6d6466a5-f977-4e16-9e89-bd3093d1f6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika) (67.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tika) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2024.6.2)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=b13672a0fdfce2832179eab99cb778ae47c1023d584c4a25eab1153d0780f2a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tika"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "g_n2tuViohoO",
        "outputId": "ec34fb47-a7dc-4f89-a65c-595b7f3ceed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-08 12:34:56,113 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
            "2024-07-08 12:34:57,074 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2024-07-08 12:34:57,777 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n",
            "2024-07-08 12:35:02,793 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n",
            "2024-07-08 12:35:07,808 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n",
            "2024-07-08 12:35:12,819 [MainThread  ] [ERROR]  Tika startup log message not received after 3 tries.\n",
            "ERROR:tika.tika:Tika startup log message not received after 3 tries.\n",
            "2024-07-08 12:35:12,827 [MainThread  ] [ERROR]  Failed to receive startup confirmation from startServer.\n",
            "ERROR:tika.tika:Failed to receive startup confirmation from startServer.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unable to start Tika server.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-65c4b0a3560c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtika\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'/content/Harshita Anala resume_07_2024.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfile_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tika/parser.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(filename, serverEndpoint, service, xmlContent, headers, config_path, requestOptions, raw_response)\u001b[0m\n\u001b[1;32m     38\u001b[0m     '''\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxmlContent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserverEndpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequestOptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequestOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         output = parse1(service, filename, serverEndpoint, services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta/xml'},\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tika/tika.py\u001b[0m in \u001b[0;36mparse1\u001b[0;34m(option, urlOrPath, serverEndpoint, verbose, tikaServerJar, responseMimeType, services, rawResponse, headers, config_path, requestOptions)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Accept'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponseMimeType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Content-Disposition'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmake_content_disposition_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0municode_string\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murlOrPath\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_file_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlOrPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         status, response = callServer('put', serverEndpoint, service, f,\n\u001b[0m\u001b[1;32m    338\u001b[0m                                       \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtikaServerJar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                                       rawResponse=rawResponse, requestOptions=requestOptions)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tika/tika.py\u001b[0m in \u001b[0;36mcallServer\u001b[0;34m(verb, serverEndpoint, service, data, headers, verbose, tikaServerJar, httpVerbs, classpath, rawResponse, config_path, requestOptions)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mTikaClientOnly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTikaClientOnly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mserverEndpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckTikaServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserverHost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtikaServerJar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0mserviceUrl\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mserverEndpoint\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tika/tika.py\u001b[0m in \u001b[0;36mcheckTikaServer\u001b[0;34m(scheme, serverHost, port, tikaServerJar, classpath, config_path)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to receive startup confirmation from startServer.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to start Tika server.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mserverEndpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to start Tika server."
          ]
        }
      ],
      "source": [
        "from tika import parser\n",
        "file = r'/content/Harshita Anala resume_07_2024.pdf'\n",
        "file_data = parser.from_file(file)\n",
        "text = file_data['content']\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hddwbirJp-my",
        "outputId": "5c6eb681-8e31-46fd-e343-07c73966a027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZe8sLMrp_LQ",
        "outputId": "2be3e4c2-1057-4595-d77b-5effda97416b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harshita Anala harshitaanala@gmail.com\n",
            "AI DS Undergraduate +91-9967511212\n",
            "Vivekanand Education Society Institute of Technology linkedin.com/in/harshita-anala-ba8229228/\n",
            "EDUCATION\n",
            "Degree Specialization Institute Year CPI\n",
            "B.Tech Artificial intelligence & Data Science VESIT 2020-2024 9.25\n",
            "HSC CBSE Physics, Chemistry, & Mathematics Jaipuriar School 2020 85%\n",
            "SSC CBSE - Akshara School 2018 94%\n",
            "PROJECTS\n",
            "•Predictive Maintenance Of Servers [Major Project] May’23-Apr’24\n",
            "◦Developed a predictive maintenance system to forecast server failures , addressing frequent down\n",
            "times impacting operations.\n",
            "◦Implemented ML models, analyzed real-time data, and created a stakeholder dashboard .Achieved\n",
            "87% accuracy in predictions, reducing server downtime and improving operational efficiency.\n",
            ". •E-Takraar [Cyber crime and abuse complaint registration portal]\n",
            "◦Faced with a high volume of unclassified messages, comments, and emails, the task was to create a\n",
            "system to classify content as legitimate or illegal.\n",
            "◦Built a website utilizing natural language processing and machine learning for model training, which\n",
            "streamlined complaint registration and enhanced efficiency and accuracy in identifying illegal\n",
            "content.\n",
            "•Multiple Disease Detection Using Scans [Detection of eye brain and kidney diseases]\n",
            "◦To address the need for affordable and accurate disease detection , the task was to develop a website\n",
            "capable of detecting brain, kidney, and eye diseases through scans without manual intervention.\n",
            "◦Built the site with diagnostic capabilities for disease stages and intensity using convolutional neural\n",
            "networks, TensorFlow and scikit-learn , along with image processing technologies like OpenCV .\n",
            "◦Achieved 90% accuracy , enabling affordable self-diagnosis and efficient disease detection.\n",
            "WORK EXPERIENCE\n",
            "•Junior Intern Trainee [Reliance JIO-RCP] May’23-June’23\n",
            "◦Developed a chatbot by training it on customer complaint data of 150k data points to solve the\n",
            "complaints of JIO customers .\n",
            "◦Participated actively in numerous technical discussions and collaborated on brainstorming sessions to\n",
            "generate innovative ideas.\n",
            "•ML Intern [VESIT Renaissance Cell] Dec’22-Jan’23\n",
            "◦Developed a user-centric disease diagnosis website using ML for symptom-based analysis.\n",
            "◦Learnt various Deep Learning and ML technologies and various techniques to improve the accuracy.\n",
            "POSITIONS OF RESPONSIBILITY\n",
            "•Public Relations Officer [VESIT AI Club] May’22-May’23\n",
            "◦Optimized participation in hackathons through strategic planning and promotion, while also\n",
            "spearheading three technical workshops to elevate professional skills as a Public Relations Officer.\n",
            "•Captain [VESIT Throwball Team] May’23-May’24\n",
            "EXTRA-CURRICULAR ACHIEVEMENTS/ACTIVITIES\n",
            "• Secured first position in Throwball tournament intercollegiate out of 10 teams as a part of Hashu Advani\n",
            "Memorial Cup 2024.\n",
            "• Secured a third position in SYRUS Technical Hackathon out of 150+ competing teams from the institute.\n",
            "• Secured first runner-up position in Technology Day (Most Innovative Minds) from the AI DS department.\n",
            "•4-star rating on HackerRank for SQL proficiency.\n",
            "• Level 3 @CodingNinja.\n",
            "TECHNICAL SKILLS\n",
            "•Languages & Technolgies: Python, SQL, Excel, Tableau, Power BI, Java, C, HTML, CSS, Javascript, R Pro-\n",
            "gramming, Machine Learning, Deep Learning, Artificial Intelligence, Data Structures, Operating system,\n",
            "DBMS, Statistics, AWS.\n"
          ]
        }
      ],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "pdf_file_path = \"/content/Harshita Anala resume_07_2024.pdf\"\n",
        "\n",
        "file_data = parser.from_file(pdf_file_path)\n",
        "text = file_data['content']\n",
        "\n",
        "# Define the path to your PDF file\n",
        "pdf_file_path = r'/content/Harshita Anala resume_07_2024.pdf'\n",
        "\n",
        "# Create a PdfReader object\n",
        "pdf_reader = PdfReader(pdf_file_path)\n",
        "\n",
        "# Initialize an empty string to store the extracted text\n",
        "extracted_text = ''\n",
        "\n",
        "# Loop through each page in the PDF\n",
        "for page in pdf_reader.pages:\n",
        "    # Extract text from the current page\n",
        "    page_text = page.extract_text()\n",
        "\n",
        "    # Append the page text to the extracted text\n",
        "    extracted_text += page_text\n",
        "\n",
        "# Print the extracted text\n",
        "print(extracted_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwIKJAfhrhlo"
      },
      "outputs": [],
      "source": [
        "text = text + \"1234567891\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3UNTT6Yr3Tr"
      },
      "outputs": [],
      "source": [
        "extracted_text = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAJD7v6Vr_GJ",
        "outputId": "736a58f0-eea9-4547-ab90-007e42702799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['harshitaanala@gmail.com']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def get_email_addresses(string):\n",
        "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
        "    return r.findall(string)\n",
        "\n",
        "email = get_email_addresses(text)\n",
        "print(email)\n",
        "\n",
        "extracted_text[\"E-Mail\"] = email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeyV0UeYsJeb",
        "outputId": "eb0ddae2-1b11-4a87-a2eb-5cc0cf97825a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['9967511212', '8229228', '0202024', '1234567891']\n"
          ]
        }
      ],
      "source": [
        "def get_phone_numbers(string):\n",
        "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
        "    phone_numbers = r.findall(string)\n",
        "    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n",
        "\n",
        "phone_number= get_phone_numbers(text)\n",
        "print(phone_number)\n",
        "\n",
        "extracted_text[\"Phone Number\"] = phone_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvSBMUqQsX7d",
        "outputId": "280ba957-1fbd-48da-a7d7-581d14a7ceac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nlp) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (14.0.2)\n",
            "Collecting dill (from nlp)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nlp) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from nlp) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from nlp) (3.15.4)\n",
            "Collecting xxhash (from nlp)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->nlp) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, nlp\n",
            "Successfully installed dill-0.3.8 nlp-0.4.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXrC3jAssaBy",
        "outputId": "1248ec14-8582-42e7-b4b3-0f99a7c52b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overleaf Example\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# initialize matcher with a vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # First name and Last name are always Proper Nouns\n",
        "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "\n",
        "    matcher.add('NAME', [pattern], on_match = None)\n",
        "\n",
        "    matches = matcher(nlp_text)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_text[start:end]\n",
        "        return span.text\n",
        "\n",
        "name = extract_name(text)\n",
        "print(name)\n",
        "extracted_text[\"Name\"] = name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY4Ig745s1Mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03996d3-2d99-403c-83ff-040d12e7ade5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deep learning',\n",
              " 'Ai',\n",
              " 'Artificial intelligence',\n",
              " 'Css',\n",
              " 'Natural language processing',\n",
              " 'Machine learning',\n",
              " 'Tensorflow',\n",
              " 'Data structures',\n",
              " 'Html',\n",
              " 'Python',\n",
              " 'Sql',\n",
              " 'Opencv']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "def extract_skills(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # removing stop words and implementing word tokenization\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "\n",
        "    skills = [\"machine learning\",\n",
        "             \"deep learning\",\n",
        "             \"nlp\",\n",
        "             \"natural language processing\",\n",
        "             \"mysql\",\n",
        "             \"sql\",\n",
        "             \"django\",\n",
        "             \"computer vision\",\n",
        "              \"tensorflow\",\n",
        "             \"opencv\",\n",
        "             \"mongodb\",\n",
        "             \"artificial intelligence\",\n",
        "             \"ai\",\n",
        "             \"flask\",\n",
        "             \"robotics\",\n",
        "             \"data structures\",\n",
        "             \"python\",\n",
        "             \"c++\",\n",
        "             \"matlab\",\n",
        "             \"css\",\n",
        "             \"html\",\n",
        "             \"github\",\n",
        "             \"php\"]\n",
        "\n",
        "    skillset = []\n",
        "\n",
        "    # check for one-grams (example: python)\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "\n",
        "    # check for bi-grams and tri-grams (example: machine learning)\n",
        "    for token in nlp_text.noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "\n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
        "\n",
        "skills = []\n",
        "skills = extract_skills(text)\n",
        "\n",
        "extracted_text[\"Skills\"] = skills\n",
        "skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StWySIE-tEI3",
        "outputId": "59742bf8-7c5e-4fbf-ca3c-d9eeb739c0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Grad all general stop words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "EDUCATION = [\n",
        "            'BE','B.E.', 'B.E', 'BS', 'B.S',\n",
        "            'ME', 'M.E', 'M.E.', 'MS', 'M.S',\n",
        "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH',\n",
        "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
        "        ]\n",
        "\n",
        "def extract_education(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # Sentence Tokenizer\n",
        "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
        "\n",
        "    edu = {}\n",
        "    # Extract education degree\n",
        "    for index, text in enumerate(nlp_text):\n",
        "        for tex in text.split():\n",
        "            # Replace all special symbols\n",
        "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
        "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
        "                edu[tex] = text + nlp_text[index + 1]\n",
        "\n",
        "    education = []\n",
        "    for key in edu.keys():\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
        "        if year:\n",
        "            education.append((key, ''.join(year[0])))\n",
        "        else:\n",
        "            education.append(key)\n",
        "    return education\n",
        "\n",
        "\n",
        "education = extract_education(text)\n",
        "education\n",
        "extracted_text[\"Qualification\"] = education"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UMItRrDtNq-",
        "outputId": "ce379dd7-a469-4cb3-810c-30376e76fbd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'E-Mail': ['harshitaanala@gmail.com'],\n",
              " 'Phone Number': ['9967511212', '8229228', '0202024', '1234567891'],\n",
              " 'Name': 'Overleaf Example',\n",
              " 'Skills': ['Deep learning',\n",
              "  'Ai',\n",
              "  'Artificial intelligence',\n",
              "  'Css',\n",
              "  'Natural language processing',\n",
              "  'Machine learning',\n",
              "  'Tensorflow',\n",
              "  'Data structures',\n",
              "  'Html',\n",
              "  'Python',\n",
              "  'Sql',\n",
              "  'Opencv'],\n",
              " 'Qualification': [('BTech', '2020'),\n",
              "  ('HSC', '2020'),\n",
              "  ('CBSE', '2020'),\n",
              "  ('SSC', '2020')]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "extracted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m1Wx4fOtP5Z",
        "outputId": "05ea336d-58c9-4b0a-e72a-e836f96fc56f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Society Institute of Technology']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import re\n",
        "sub_patterns = ['[A-Z][a-z]* University','[A-Z][a-z]* Educational Institute',\n",
        "                'University of [A-Z][a-z]*',\n",
        "                '[A-Z][a-z]* Institute of Technology' ,\n",
        "                'Ecole [A-Z][a-z]*']\n",
        "pattern = '({})'.format('|'.join(sub_patterns))\n",
        "matches = re.findall(pattern, text)\n",
        "\n",
        "extracted_text[\"Institutes\"] = matches\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpjwpW1etSUL",
        "outputId": "26465cc9-845f-4dde-aca5-d102360b8a12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import re\n",
        "sub_patterns = ['[A-Z][a-z]* [A-Z][a-z]* Private Limited','[A-Z][a-z]* [A-Z][a-z]* Pvt. Ltd.','[A-Z][a-z]* [A-Z][a-z]* Inc.', '[A-Z][a-z]* LLC',\n",
        "                ]\n",
        "pattern = '({})'.format('|'.join(sub_patterns))\n",
        "Exp = re.findall(pattern, text)\n",
        "extracted_text[\"Experience\"] = Exp\n",
        "Exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-naIxV0ZtUMw",
        "outputId": "608f650d-5130-4947-9f2a-8241529965c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'E-Mail': ['harshitaanala@gmail.com'],\n",
              " 'Phone Number': ['9967511212', '8229228', '0202024', '1234567891'],\n",
              " 'Name': 'Overleaf Example',\n",
              " 'Skills': ['Deep learning',\n",
              "  'Ai',\n",
              "  'Artificial intelligence',\n",
              "  'Css',\n",
              "  'Natural language processing',\n",
              "  'Machine learning',\n",
              "  'Tensorflow',\n",
              "  'Data structures',\n",
              "  'Html',\n",
              "  'Python',\n",
              "  'Sql',\n",
              "  'Opencv'],\n",
              " 'Qualification': [('BTech', '2020'),\n",
              "  ('HSC', '2020'),\n",
              "  ('CBSE', '2020'),\n",
              "  ('SSC', '2020')],\n",
              " 'Institutes': [],\n",
              " 'Experience': []}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "extracted_text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}